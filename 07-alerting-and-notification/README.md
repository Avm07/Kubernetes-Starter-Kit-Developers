# Alerting and Notification

## Table of contents

- [Alerting and Notification Overview](#alerting-and-notification-overview)
- [List of Included Alerts](#list-of-included-alerts)
- [Creating a New Alert](#creating-a-new-alert)
- [Configuring Alertmanager to Send Notification to Slack](#configuring-alertmanager-to-send-notification-to-slack)

## Alerting and Notification Overview

Often times we need to be notified immediately about any critical issue in our cluster. That is where `AlertManager` comes into the picture. `Alertmanager` helps in aggregating the `alerts`, and sending `notifications` as shown in the diagram below.

![AlertManager Overview](assets/images/alert-manager-overview.png)

Alertmanager, usually deployed alongside Prometheus, forms the alerting layer of the `kube-prom-stack`, handling alerts generated by Prometheus and deduplicating, grouping, and routing them to integrations like email, Slack or PagerDury.
Alerts and Notifications are a critical part of your workflow. When things go wrong (e.g. any service is down, or a pod is crashing etc.), you expect to see the appropriate alerts, and notifications to handle these situations. The Starter Kit includes a set of default alerts as part of the `kube-prom-stack` installation.

Alertmanager is part of the `kube-prom-stack` installed in your cluster in [Chapter 4 - 04-setup-prometheus-stack](../04-setup-prometheus-stack/README.md). For this tutorial you will be using the same manifest file used in `Chapter 4`. `AlertManager` allows you to receive alerts from various clients (sources), like `Prometheus` for example. Rules are created on the `Prometheus` side, which in turn can fire alerts. Then, it’s the responsibility of `AlertManager` to intercept those alerts, group them (aggregation), apply other transformations and finally route to the configured receivers. Notification messages can be further formatted to include additional details if desired. You can use Slack, Gmail, etc to send real time notifications.

In this section, you will learn how to inspect the existing alerts, create new ones, and then configure `AlertManager` to send notifications via `Slack`.

## List of Included Alerts

Kube-prom-stack has over a hundred rules already activated.
To access the prometheus console, first do a port-forward to your local machine.

```shell
kubectl --namespace monitoring port-forward svc/kube-prom-stack-kube-prome-prometheus 9091:9090
```

Open a web browser on [localhost:9091](http://localhost:9091) and access the `Alerts` menu item. The following is a sample set of alerts from that list.

- *TargetDown:* One or more targets are unreachable or down.
- *Watchdog :* An alert that should always be firing to certify that `Alertmanager` is working properly.
- *KubePodCrashLooping:* when Pod is crash-looping, Pod is restarting some times / 10 minutes. Also `AlertManager` creates an alert.
- *KubePodNotReady:* An alert that Pod has been in a non-ready state for longer than 15 minutes.
- *KubeDeploymentReplicasMismatch:* when a deployment has not matched the expected number of replicas, AlertManager is creating this alert.
- *KubeStatefulSetReplicasMismatch:* when StatefulSet has not matched the expected number of replicas for longer than 15 minutes, AlertManager creates an alert.
- *KubeJobCompletion:* when a job is taking more than 12 hours to complete, Job did not complete in time
- *KubeJobFailed:* when a job failed to complete. Removing a failed job after investigation should clear this alert.
- *CPUThrottlingHigh:* when you reach CPU Limits, CPUthrottling is firing this rule to urge you.
- *KubeControllerManagerDown:* kube-controller-manager is a kind of job that helps kube-controller-manager run or not when it has disappeared from Prometheus target discovery. this rule will detect this unwanted situation.
- *KubeSchedulerDown:* kube-scheduler is an important part of kubernetes. This job is searching for a kube-scheduler that is running when it is absent. This rule helps us to understand it’s health.

## Creating a New Alert

To create a new alert, you need to add the alert definition under `additionalPrometheusRules` section in the manifest file.
You will be creating a sample alert that will trigger if the `Emojitovo` namespace does not have an expected number of instances. The expected number of pods for the `Emojivoto` application is 4.
First, open the `04-setup-prometheus-stack/assets/manifests/prom-stack-values.yaml` file provided in the `Starter Kit` repository, using a text editor of your choice (preferably with `YAML` lint support). Uncomment the `additionalPrometheusRules` block.

```yaml
additionalPrometheusRulesMap:
  rule-name:
   groups:
   - name: emojivoto-instance-down
     rules:
      - alert: EmojivotoInstanceDown
        expr: sum(kube_pod_owner{namespace="emojivoto"}) by (namespace) < 4
        for: 1m
        labels:
          severity: 'critical'
  annotations:
          title: 'Instance {{ $labels.instance }} down'
          description: 'Emojivoto pod(s) is down for more than 1 minute.'
```

Finally, apply settings using `Helm`:

  ```shell
  HELM_CHART_VERSION="35.5.1"

  helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version "${HELM_CHART_VERSION}" \
    --namespace monitoring \
    -f "04-setup-prometheus-stack/assets/manifests/prom-stack-values-v${HELM_CHART_VERSION}.yaml"
  ```

## Configuring Alertmanager to Send Notification to Slack

To complete this section you need to have administrative rights over a workspace. This will enable you to create the incoming webhook you will need in the next steps. You will also need to create a channel where you would like to receive notifications from `AlertManager`. This is not required but it is a good practice to have separate channels for alerts and notifications.

Steps to follow:

1. From the `Slack` desktop or web application, select the `Administration` --> `Manage Apps`.
2. In the `Manage apps` directory, search for `Incoming WebHooks` in the search bar and click on the `Add to Slack button.
3. In the `Post to Channel` section choose a channel you want AlertManager to send notifications to.
4. On the next page, a `Webhook URL` will be displeyd. Make sure to copy it as you will use it in the next steps.
5. Click on the `Save Settings` button at the bottom of the page.

Next, we will add the necessary config to `AlertManager` to enable `Slack` notifications. Open the `04-setup-prometheus-stack/assets/manifests/prom-stack-values.yaml` file provided in the `Starter Kit` repository, using a text editor of your choice (preferably with `YAML` lint support). Uncomment the `config` block. Make sure to update the `slack_api_url` and `channel` values.

```yaml
alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
      slack_api_url: "https://hooks.slack.com/services/TXXXX/BXXXX/<token>"
    route:
      repeat_interval: 1h
      routes:
      - match:
          alertname: EmojivotoInstanceDown
        receiver: 'slack'
        continue: true
    Receivers:
    - name: 'slack'
      slack_configs:
      - channel: '#channelName'
        send_resolved: false
        title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification'
        text: "Emojivoto number of pods less than expected."
```

Explanations for the above configuration:

- `slack_api_url` - url of the `webhook` created in step 4.
- `routes.match.alertname` - name of the `AlertManager` alert.
- `slack_configs` - configuration related to the channel to send the notification to, the title and the message it should include.

Finally, upgrade the `kube-prometheus-stack`, using `Helm`:

  ```shell
  HELM_CHART_VERSION="35.5.1"

  helm upgrade kube-prom-stack prometheus-community/kube-prometheus-stack --version "${HELM_CHART_VERSION}" \
    --namespace monitoring \
    --create-namespace \
    -f "04-setup-prometheus-stack/assets/manifests/prom-stack-values-v${HELM_CHART_VERSION}.yaml"
  ```

At this point, you should see the slack `notifications` for firing alerts when the expected number of pods for `Emojivoto` is less than 4.

Go to [Section 08 - Encrypt Kubernetes Secrets Using Sealed Secrets](../08-kubernetes-sealed-secrets/README.md).
